{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Introduction to NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus- A large collection of text\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())\n",
    "print(len(brown.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = brown.sents(categories='adventure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"He certainly didn't want a wife who was fickle as Ann .\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.ConcatenatedCorpusView"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[2]),len(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Pipeline\n",
    "* Get the Data/Corpus\n",
    "* Tokenization - Getting sentences and words out of Documents, Stopwords Removal - Removal of words       like was, the, is, etc.\n",
    "* Stemming - Matching a pool of similar words to some base word.\n",
    "* Building a Vocab - Creating a pool of unique words\n",
    "* Vectorization - Forming a vector which serves as a feature to our classification algorithm.\n",
    "* Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "document  = \"\"\"It was a pleasant day. The weather was cool and there were light showers. I went to the\n",
    "market to buy some cigarettes.\"\"\"\n",
    "\n",
    "sentence = \"Send all the 50 documents related to chapters 1,2,3 at prateekbhaiya@cid.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a pleasant day.', 'The weather was cool and there were light showers.', 'I went to the\\nmarket to buy some cigarettes.']\n"
     ]
    }
   ],
   "source": [
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was a pleasant day.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " '1,2,3',\n",
       " 'at',\n",
       " 'prateekbhaiya@cid.com']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'related', 'to', 'chapters', '1,2,3', 'at', 'prateekbhaiya', '@', 'cid.com']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isn', 's', \"wasn't\", 'whom', 'such', 'or', 'then', 'through', 'did', 'not', 'only', 'when', 'you', 't', 'ours', 'your', 'they', 'aren', 'too', 'hers', 'an', 'myself', 'she', 'have', 'can', 'now', 'here', 'him', 'just', 'been', \"don't\", 'mustn', \"that'll\", 'my', 'because', 'as', \"you're\", 'no', \"couldn't\", 'be', 'our', 'into', 'very', 'll', 'for', 'with', 'wouldn', 'other', 'these', 'against', 'same', 'above', 'why', 'had', 'ain', 'y', \"hadn't\", 'was', 'if', 'more', 'hadn', 'of', 'that', 'below', 'ourselves', 'most', 'doesn', 'out', 'before', 'from', 'how', 'o', 'its', 'haven', \"weren't\", 'he', 'her', 'nor', 'and', 'will', 'any', 'on', \"doesn't\", 'further', \"hasn't\", 'itself', 'all', 'to', 'am', 'i', 'being', \"you'll\", 'those', \"mustn't\", 'his', 'but', 'in', 'down', 'until', 'herself', 'over', 'some', 'than', \"mightn't\", \"haven't\", 'by', \"needn't\", 'so', 'couldn', 'the', 'under', 'few', \"should've\", 'hasn', 'off', 'this', 'mightn', \"won't\", 'needn', 'yourself', 'after', \"she's\", 'them', 'm', 'd', 'again', \"shouldn't\", 'do', 'yourselves', 'there', 'has', 've', \"shan't\", 'shouldn', 'does', 'shan', 'a', 'didn', \"wouldn't\", 'where', \"you've\", 'himself', \"didn't\", 'me', 'it', 'should', 'once', \"you'd\", 'about', 'are', 'don', 'both', 'own', 'during', 'at', 're', 'weren', 'is', 'between', 'yours', 'doing', \"aren't\", 'ma', 'we', 'their', 'having', 'themselves', 'what', 'each', 'which', \"isn't\", 'were', 'who', 'wasn', 'theirs', 'while', 'up', 'won', \"it's\"}\n"
     ]
    }
   ],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc,stopwords):\n",
    "    useful_words = [w for w in doc if w not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"Please don't even try to bother me!\"\n",
    "useful_words = remove_stopwords(doc.split(),sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please', 'even', 'try', 'bother', 'me!']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'try' in sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization with Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Send all the 50 documents related to chapters 1,2,3 at prateekbhaiya@cid.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('[a-zA-Z@.]+')\n",
    "useful_text = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " 'at',\n",
       " 'prateekbhaiya@cid.com']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "* Process that transforms particular words(verbs,plurals) into their radical form.\n",
    "* Preserve the semantics of the sentence without increasing the number of unique tokens.\n",
    "* Example : jumps, jumping, jumped, jump ==> jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer,PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('lovely')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmatize('jumps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Vocab and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Corpus -  contains 4 documents, each document can have 1 or more sentences. \n",
    "corpus = [\n",
    "    \"Indian cricket team is about to win this game says Capt. Virat Kohli.\",\n",
    "    \"All the leaders of Congress and BJP admits that they all are of no use of the public.\",\n",
    "    \"The nobel prize will be given to the man who saved his neighbour's wife from drowning.\",\n",
    "    \"The movie 3-idiots still tops the list of my favourite movies.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x48 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 52 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "        0, 0, 0, 1],\n",
       "       [0, 1, 2, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 3, 0, 1, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian': 17,\n",
       " 'cricket': 9,\n",
       " 'team': 35,\n",
       " 'is': 18,\n",
       " 'about': 0,\n",
       " 'to': 40,\n",
       " 'win': 47,\n",
       " 'this': 39,\n",
       " 'game': 13,\n",
       " 'says': 33,\n",
       " 'capt': 7,\n",
       " 'virat': 43,\n",
       " 'kohli': 19,\n",
       " 'all': 2,\n",
       " 'the': 37,\n",
       " 'leaders': 20,\n",
       " 'of': 29,\n",
       " 'congress': 8,\n",
       " 'and': 3,\n",
       " 'bjp': 6,\n",
       " 'admits': 1,\n",
       " 'that': 36,\n",
       " 'they': 38,\n",
       " 'are': 4,\n",
       " 'no': 27,\n",
       " 'use': 42,\n",
       " 'public': 31,\n",
       " 'nobel': 28,\n",
       " 'prize': 30,\n",
       " 'will': 46,\n",
       " 'be': 5,\n",
       " 'given': 14,\n",
       " 'man': 22,\n",
       " 'who': 44,\n",
       " 'saved': 32,\n",
       " 'his': 15,\n",
       " 'neighbour': 26,\n",
       " 'wife': 45,\n",
       " 'from': 12,\n",
       " 'drowning': 10,\n",
       " 'movie': 23,\n",
       " 'idiots': 16,\n",
       " 'still': 34,\n",
       " 'tops': 41,\n",
       " 'list': 21,\n",
       " 'my': 25,\n",
       " 'favourite': 11,\n",
       " 'movies': 24}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 17, 'cricket': 9, 'team': 35, 'is': 18, 'about': 0, 'to': 40, 'win': 47, 'this': 39, 'game': 13, 'says': 33, 'capt': 7, 'virat': 43, 'kohli': 19, 'all': 2, 'the': 37, 'leaders': 20, 'of': 29, 'congress': 8, 'and': 3, 'bjp': 6, 'admits': 1, 'that': 36, 'they': 38, 'are': 4, 'no': 27, 'use': 42, 'public': 31, 'nobel': 28, 'prize': 30, 'will': 46, 'be': 5, 'given': 14, 'man': 22, 'who': 44, 'saved': 32, 'his': 15, 'neighbour': 26, 'wife': 45, 'from': 12, 'drowning': 10, 'movie': 23, 'idiots': 16, 'still': 34, 'tops': 41, 'list': 21, 'my': 25, 'favourite': 11, 'movies': 24}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reverse Mapping - Making sentences from these feature matrix\n",
    "\n",
    "numbers = vectorized_corpus[2]\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = cv.inverse_transform(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['be', 'drowning', 'from', 'given', 'his', 'man', 'neighbour',\n",
       "       'nobel', 'prize', 'saved', 'the', 'to', 'who', 'wife', 'will'],\n",
       "      dtype='<U9')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['be', 'drowning', 'from', 'given', 'his', 'man', 'neighbour',\n",
      "       'nobel', 'prize', 'saved', 'the', 'to', 'who', 'wife', 'will'],\n",
      "      dtype='<U9')]\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization with Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(doc):\n",
    "    words = tokenizer.tokenize(doc.lower())\n",
    "#   Remove stopwords\n",
    "    words = remove_stopwords(words,sw)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send', 'documents', 'related', 'chapters', 'prateekbhaiya@cid.com']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1]\n",
      " [1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['capt.', 'cricket', 'game', 'indian', 'kohli.', 'says', 'team',\n",
       "        'virat', 'win'], dtype='<U9'),\n",
       " array(['admits', 'bjp', 'congress', 'leaders', 'public.', 'use'],\n",
       "       dtype='<U9'),\n",
       " array(['drowning.', 'given', 'man', 'neighbour', 'nobel', 'prize',\n",
       "        'saved', 'wife'], dtype='<U9'),\n",
       " array(['favourite', 'idiots', 'list', 'movie', 'movies.', 'still', 'tops'],\n",
       "       dtype='<U9')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Using on Test corpus\n",
    "test_corpus = [\n",
    "    \"India is going to rock!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember for test corpus use only transform function not fit_transform\n",
    "cv.transform(test_corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian': 10,\n",
       " 'cricket': 4,\n",
       " 'team': 24,\n",
       " 'win': 29,\n",
       " 'game': 7,\n",
       " 'says': 22,\n",
       " 'capt.': 2,\n",
       " 'virat': 27,\n",
       " 'kohli.': 11,\n",
       " 'leaders': 12,\n",
       " 'congress': 3,\n",
       " 'bjp': 1,\n",
       " 'admits': 0,\n",
       " 'use': 26,\n",
       " 'public.': 20,\n",
       " 'nobel': 18,\n",
       " 'prize': 19,\n",
       " 'given': 8,\n",
       " 'man': 14,\n",
       " 'saved': 21,\n",
       " 'neighbour': 17,\n",
       " 'wife': 28,\n",
       " 'drowning.': 5,\n",
       " 'movie': 15,\n",
       " 'idiots': 9,\n",
       " 'still': 23,\n",
       " 'tops': 25,\n",
       " 'list': 13,\n",
       " 'favourite': 6,\n",
       " 'movies.': 16}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words - Bigrams, Trigrams, Ngrams\n",
    "* Unigram - Every word as a feature\n",
    "* Bigrams\n",
    "* Trigrams\n",
    "* Ngrams\n",
    "* TF-IDF Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = [\"This is a good movie\"]\n",
    "sent_2 = [\"This is a good movie but the actor was not that good\"]\n",
    "sent_3 = [\"This is not a good movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2)) # Here changing (2,2) to (1,3) or (2,3) or any other combination \n",
    "# will them Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [sent_1[0],sent_2[0],sent_3[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 10,\n",
       " 'is good': 3,\n",
       " 'good movie': 2,\n",
       " 'movie but': 5,\n",
       " 'but the': 1,\n",
       " 'the actor': 9,\n",
       " 'actor was': 0,\n",
       " 'was not': 11,\n",
       " 'not that': 7,\n",
       " 'that good': 8,\n",
       " 'is not': 4,\n",
       " 'not good': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Normalization - Term Document Frequency-Inverse Document Frequency\n",
    "* Avoid features that occur very often or in almost all type of documents like sports, politics etc.\n",
    "  as they contain very less information.\n",
    "* Information decreases as the no. of occurences increases across diff. types of documents.\n",
    "* So we define another term - TF-IDf which associates a weight with every term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = [\"This is a good movie\"]\n",
    "sent_2 = [\"That was a good movie\"]\n",
    "sent_3 = [\"This is not a good movie\"]\n",
    "\n",
    "corpus = [sent_1[0],sent_2[0],sent_3[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 5, 'is': 1, 'good': 0, 'movie': 2, 'that': 4, 'was': 6, 'not': 3}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43370786 0.55847784 0.43370786 0.         0.         0.55847784\n",
      "  0.        ]\n",
      " [0.35959372 0.         0.35959372 0.         0.6088451  0.\n",
      "  0.6088451 ]\n",
      " [0.34957775 0.45014501 0.34957775 0.59188659 0.         0.45014501\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
